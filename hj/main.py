import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.tools import Tool
from langchain.memory import ConversationBufferMemory
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from utils import render_with_latex
from sidebar import render_sidebar
from database import Database
import os
import json

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”

# ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# í˜„ì¬ ëŒ€í™” ì„¸ì…˜ IDê°€ ì—†ìœ¼ë©´ Noneë¡œ ì´ˆê¸°í™”
if "current_conversation_id" not in st.session_state:
    st.session_state.current_conversation_id = None

# API í‚¤ ìœ íš¨ì„± ê²€ì¦ ìƒíƒœê°€ ì—†ìœ¼ë©´ Falseë¡œ ì´ˆê¸°í™”
if "api_key_valid" not in st.session_state:
    st.session_state.api_key_valid = False

# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
db = Database()

# Streamlit ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="HealthAgent", page_icon=":climbing:", layout="wide")
st.title("ğŸ”ï¸ Health Agent")

# ì‚¬ì´ë“œë°” ë Œë”ë§
render_sidebar()

# API í‚¤ ìœ íš¨ì„± ê²€ì¦ ìƒíƒœ í™•ì¸
if not st.session_state.api_key_valid:
    st.warning("OpenAI API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ìœ íš¨í•œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop()  # ì—¬ê¸°ì„œ ì‹¤í–‰ì„ ì¤‘ë‹¨í•˜ì—¬ ì±„íŒ… ê¸°ëŠ¥ ì œí•œ

# LangChain ì„¤ì •
try:
    api_key = st.session_state.get("openai_api_key", st.secrets.get("openai", {}).get("api_key", ""))
except Exception:
    api_key = st.session_state.get("openai_api_key", "")

if not api_key:
    st.warning("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop()

# LLM ì„¤ì •
llm = ChatOpenAI(
    api_key=api_key,
    model="gpt-4o-mini",
    temperature=0.7,
    streaming=True
)

# ì„ë² ë”© ì„¤ì •
embeddings = OpenAIEmbeddings(api_key=api_key)

# ë©”ëª¨ë¦¬ ì„¤ì •
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ê±´ê°• ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì „ë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤. 
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
    ê°€ëŠ¥í•œ í•œ ìì„¸í•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    """),
    ("human", "{input}")
])

# ì—ì´ì „íŠ¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
agent_prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ê±´ê°• ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì „ë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
    ê°€ëŠ¥í•œ í•œ ìì„¸í•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    
    ì‚¬ìš© ê°€ëŠ¥í•œ íˆ´:
    {tools}
    
    ì‚¬ìš© ë°©ë²•:
    1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì´í•´í•©ë‹ˆë‹¤.
    2. í•„ìš”í•œ ê²½ìš° ì ì ˆí•œ íˆ´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    3. íˆ´ì˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
    
    {agent_scratchpad}
    """),
    ("human", "{input}")
])

# ì»¤ìŠ¤í…€ íˆ´ ì •ì˜ ì˜ˆì‹œ
def search_health_info(query: str) -> str:
    """ê±´ê°• ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” íˆ´"""
    # ì—¬ê¸°ì— ì‹¤ì œ ê²€ìƒ‰ ë¡œì§ êµ¬í˜„
    return f"ê²€ìƒ‰ ê²°ê³¼: {query}"

# íˆ´ ì„¤ì •
tools = [
    Tool(
        name="health_search",
        func=search_health_info,
        description="ê±´ê°• ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” íˆ´ì…ë‹ˆë‹¤."
    ),
    # ì¶”ê°€ íˆ´ì„ ì—¬ê¸°ì— ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
]

# ì—ì´ì „íŠ¸ ì„¤ì •
agent = create_openai_functions_agent(llm, tools, agent_prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# RAG ì„¤ì •
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

# ë²¡í„° ìŠ¤í† ì–´ ì„¤ì • (ì‹¤ì œ ì‚¬ìš© ì‹œ ì£¼ì„ í•´ì œ)
# vectorstore = FAISS.from_texts(
#     texts=["ê±´ê°• ì •ë³´ ìƒ˜í”Œ 1", "ê±´ê°• ì •ë³´ ìƒ˜í”Œ 2"],
#     embedding=embeddings
# )

# ì²´ì¸ ì„¤ì •
chain = (
    {"input": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶œë ¥
if st.session_state.messages:
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            if msg["role"] == "assistant":
                st.markdown(render_with_latex(msg["content"]))
            else:
                st.markdown(msg["content"])

# ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

if user_input:
    
    # í˜„ì¬ ëŒ€í™” ì„¸ì…˜ IDê°€ ì—†ìœ¼ë©´ ìƒì„±; ì²« ì¿¼ë¦¬ì˜ 10ìë§Œí¼ ì œëª©ìœ¼ë¡œ ì„¤ì •
    if not st.session_state.current_conversation_id:
        conversation_title = user_input[:10] + "..." if len(user_input) > 50 else user_input
        st.session_state.current_conversation_id = db.create_conversation(conversation_title)
        
        
    # ì‚¬ìš©ìì˜ ì²« ë²ˆì§¸ ì§ˆë¬¸ì„ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    st.chat_message("user").write(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})
    db.save_message(st.session_state.current_conversation_id, "user", user_input)

    with st.chat_message("assistant"):
        stream_placeholder = st.empty()
        full_response = ""

        try:
            # ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„± (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)
            # response = agent_executor.invoke({"input": user_input})
            # full_response = response["output"]

            # ë˜ëŠ” ê¸°ë³¸ ì²´ì¸ì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±
            for chunk in chain.stream(user_input):
                full_response += chunk
                stream_placeholder.markdown(render_with_latex(full_response + "â–Œ"))

            # ìµœì¢… ì‘ë‹µ í‘œì‹œ
            stream_placeholder.empty()
            st.markdown(render_with_latex(full_response))

            # ì‘ë‹µ ì €ì¥
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            db.save_message(st.session_state.current_conversation_id, "assistant", full_response)

        except Exception as e:
            st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.info("ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            st.stop()